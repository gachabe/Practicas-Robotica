{"cells":[{"cell_type":"markdown","id":"753425f9","metadata":{"id":"753425f9"},"source":["# Práctica 6: Planificación con incertidumbre (II)\n","\n","## Inteligencia Artificial para la Robótica\n","### M.U en Lógica, Computación e Inteligencia Artificial\n","### Universidad de Sevilla"]},{"cell_type":"markdown","id":"bd63e439","metadata":{"id":"bd63e439"},"source":["En esta práctica vamos a estudiar como definir y resolver POMDPs utilizando la biblioteca [pomdp-py](https://h2r.github.io/pomdp-py/html/)"]},{"cell_type":"markdown","id":"9ce6b9ee","metadata":{"id":"9ce6b9ee"},"source":["* [Instala](https://h2r.github.io/pomdp-py/html/installation.html) la biblioteca y analiza la documentación existente en [pomdp-py](https://h2r.github.io/pomdp-py/html/)\n","* Prueba la biblioteca con los [ejemplos](https://h2r.github.io/pomdp-py/html/installation.html#test-things-out) incluidos"]},{"cell_type":"markdown","id":"b2876d62","metadata":{"id":"b2876d62"},"source":["A continuación se incluye el código del ejemplo [Tiger](https://h2r.github.io/pomdp-py/html/examples.tiger.html)\n","\n","* Analiza y prueba el código"]},{"cell_type":"code","execution_count":null,"id":"dfc017e0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfc017e0","outputId":"75314fd0-a279-4174-ab44-654948283fa9","executionInfo":{"status":"ok","timestamp":1707737127366,"user_tz":-60,"elapsed":13026,"user":{"displayName":"Gabriel ChavesBenítez","userId":"10155876556175710393"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pomdp-py\n","  Downloading pomdp_py-1.3.4-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from pomdp-py) (1.23.5)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pomdp-py) (1.11.4)\n","Requirement already satisfied: tqdm>=4.55.0 in /usr/local/lib/python3.10/dist-packages (from pomdp-py) (4.66.1)\n","Installing collected packages: pomdp-py\n","Successfully installed pomdp-py-1.3.4\n"]}],"source":["!pip install pomdp-py"]},{"cell_type":"code","execution_count":null,"id":"13048793","metadata":{"id":"13048793"},"outputs":[],"source":["import pomdp_py\n","from pomdp_py.utils import TreeDebugger\n","import random\n","import numpy as np\n","import sys"]},{"cell_type":"code","execution_count":null,"id":"7cb4ab66","metadata":{"id":"7cb4ab66"},"outputs":[],"source":["class TigerState(pomdp_py.State):\n","    def __init__(self, name):\n","        self.name = name\n","    def __hash__(self):\n","        return hash(self.name)\n","    def __eq__(self, other):\n","        if isinstance(other, TigerState):\n","            return self.name == other.name\n","        return False\n","    def __str__(self):\n","        return self.name\n","    def __repr__(self):\n","        return \"TigerState(%s)\" % self.name\n","    def other(self):\n","        if self.name.endswith(\"left\"):\n","            return TigerState(\"tiger-right\")\n","        else:\n","            return TigerState(\"tiger-left\")"]},{"cell_type":"code","execution_count":null,"id":"7901162a","metadata":{"id":"7901162a"},"outputs":[],"source":["class TigerAction(pomdp_py.Action):\n","    def __init__(self, name):\n","        self.name = name\n","    def __hash__(self):\n","        return hash(self.name)\n","    def __eq__(self, other):\n","        if isinstance(other, TigerAction):\n","            return self.name == other.name\n","        return False\n","    def __str__(self):\n","        return self.name\n","    def __repr__(self):\n","        return \"TigerAction(%s)\" % self.name"]},{"cell_type":"code","execution_count":null,"id":"24abf88f","metadata":{"id":"24abf88f"},"outputs":[],"source":["class TigerObservation(pomdp_py.Observation):\n","    def __init__(self, name):\n","        self.name = name\n","    def __hash__(self):\n","        return hash(self.name)\n","    def __eq__(self, other):\n","        if isinstance(other, TigerObservation):\n","            return self.name == other.name\n","        return False\n","    def __str__(self):\n","        return self.name\n","    def __repr__(self):\n","        return \"TigerObservation(%s)\" % self.name"]},{"cell_type":"code","source":["# Observation model: definimos las matrices de transicion\n","class ObservationModel(pomdp_py.ObservationModel):\n","    def __init__(self, noise=0.15):\n","        self.noise = noise\n","\n","    def probability(self, observation, next_state, action):\n","        if action.name == \"listen\":\n","            # heard the correct growl\n","            if observation.name == next_state.name:\n","                return 1.0 - self.noise\n","            else:\n","                return self.noise\n","        else:\n","            return 0.5\n","\n","    def sample(self, next_state, action):\n","        if action.name == \"listen\":\n","            thresh = 1.0 - self.noise\n","        else:\n","            thresh = 0.5\n","\n","        if random.uniform(0,1) < thresh:\n","            return TigerObservation(next_state.name)\n","        else:\n","            return TigerObservation(next_state.other().name)\n","\n","    def get_all_observations(self):\n","        \"\"\"Only need to implement this if you're using\n","        a solver that needs to enumerate over the observation space\n","        (e.g. value iteration)\"\"\"\n","        return [TigerObservation(s)\n","                for s in {\"tiger-left\", \"tiger-right\"}]"],"metadata":{"id":"g35Rfizyn3Z_"},"id":"g35Rfizyn3Z_","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"9ef6716b","metadata":{"id":"9ef6716b"},"outputs":[],"source":["# Transition Model: me dice que estado alcanzo al ejecutar una accion: T:SxAxS y devuelve probabilidad\n","class TransitionModel(pomdp_py.TransitionModel):\n","\n","    # probabilidad de alcanzar el estado, estando en un estado y haciendo una accion\n","    # esto es lo que se implementa en el modelo\n","    def probability(self, next_state, state, action):\n","        \"\"\"According to problem spec, the world resets once\n","        action is open-left/open-right. Otherwise, stays the same\"\"\"\n","        if action.name.startswith(\"open\"):\n","            return 0.5 # siempre devuelve una probabilidad (el tigre sigue siempre en el mismo sitio)\n","        else: # nunca llega a ser 0 o 1.\n","            if next_state.name == state.name:\n","                return 1.0 - 1e-9\n","            else:\n","                return 1e-9\n","\n","\n","    def sample(self, state, action):\n","        if action.name.startswith(\"open\"):\n","            return random.choice(self.get_all_states())\n","        else:\n","            return TigerState(state.name)\n","\n","    def get_all_states(self):\n","        \"\"\"Only need to implement this if you're using\n","        a solver that needs to enumerate over the observation space (e.g. value iteration)\"\"\"\n","        return [TigerState(s) for s in {\"tiger-left\", \"tiger-right\"}]"]},{"cell_type":"code","execution_count":null,"id":"e9eea45d","metadata":{"id":"e9eea45d"},"outputs":[],"source":["# Reward Model: SxA es la función recompensa (hay que implementarlo)\n","# reward asociado al estado\n","class RewardModel(pomdp_py.RewardModel):\n","    def _reward_func(self, state, action):\n","        if action.name == \"open-left\":\n","            if state.name == \"tiger-right\":\n","                return 10\n","            else:\n","                return -100\n","        elif action.name == \"open-right\":\n","            if state.name == \"tiger-left\":\n","                return 10\n","            else:\n","                return -100\n","        else: # listen\n","            return -1\n","\n","    # porque el reward puede ser no determinista\n","    # estoy en un estado, ejecuto accion y llego a otro estado, devuelve el reward.\n","    def sample(self, state, action, next_state):\n","        # deterministic\n","        return self._reward_func(state, action) # devuelve el reward de aplicar al estado una accion"]},{"cell_type":"code","execution_count":null,"id":"bf68ff77","metadata":{"id":"bf68ff77"},"outputs":[],"source":["# Policy Model\n","class PolicyModel(pomdp_py.RolloutPolicy):\n","    \"\"\"A simple policy model with uniform prior over a\n","       small, finite action space\"\"\"\n","\n","    ACTIONS = [TigerAction(s)\n","              for s in {\"open-left\", \"open-right\", \"listen\"}]\n","\n","\n","    def sample(self, state): # dame una accion al azar aplicable al estado\n","        return random.sample(self.get_all_actions(), 1)[0]\n","\n","\n","    # lo mismo que sample: función que ejecuta acciones hasta llegar a un estado de parada.\n","    def rollout(self, state, history=None):\n","        \"\"\"Treating this PolicyModel as a rollout policy\"\"\"\n","        return self.sample(state)\n","\n","    def get_all_actions(self, state=None, history=None): # tienen que estar todas las acciones\n","    # tenemos todas las acciones a partir de un etsdo concreto.\n","    # siempre devuelvo la misma lista\n","        return PolicyModel.ACTIONS"]},{"cell_type":"code","execution_count":null,"id":"33b7d54e","metadata":{"id":"33b7d54e"},"outputs":[],"source":["class TigerProblem(pomdp_py.POMDP):\n","    \"\"\"\n","    In fact, creating a TigerProblem class is entirely optional\n","    to simulate and solve POMDPs. But this is just an example\n","    of how such a class can be created.\n","    \"\"\"\n","\n","    def __init__(self, obs_noise, init_true_state, init_belief):\n","        \"\"\"init_belief is a Distribution.\"\"\"\n","        # separamos el agente del entorno\n","        agent = pomdp_py.Agent(init_belief,\n","                               PolicyModel(),\n","                               TransitionModel(),\n","                               ObservationModel(obs_noise),\n","                               RewardModel())\n","        env = pomdp_py.Environment(init_true_state, # le pasamos cual es el estado real (instancia de TigerState)\n","                                   TransitionModel(),\n","                                   RewardModel())\n","        super().__init__(agent, env, name=\"TigerProblem\")\n","\n","    @staticmethod\n","    def create(state=\"tiger-left\", belief=0.5, obs_noise=0.15):\n","        \"\"\"\n","        Args:\n","            state (str): could be 'tiger-left' or 'tiger-right';\n","                         True state of the environment\n","            belief (float): Initial belief that the target is\n","                            on the left; Between 0-1.\n","            obs_noise (float): Noise for the observation\n","                               model (default 0.15)\n","        \"\"\"\n","        init_true_state = TigerState(state)\n","        # crea el belief (distr de prob) con un histograma -> diccionario\n","        # cada clave es un estado\n","        init_belief = pomdp_py.Histogram({\n","            TigerState(\"tiger-left\"): belief,\n","            TigerState(\"tiger-right\"): 1.0 - belief\n","        })\n","        tiger_problem = TigerProblem(obs_noise,\n","                                     init_true_state, init_belief)\n","        tiger_problem.agent.set_belief(init_belief, prior=True)\n","        return tiger_problem"]},{"cell_type":"code","execution_count":null,"id":"1a5cb779","metadata":{"id":"1a5cb779"},"outputs":[],"source":["def test_planner(tiger_problem, planner, nsteps=3,\n","                 debug_tree=False):\n","    \"\"\"\n","    Runs the action-feedback loop of Tiger problem POMDP\n","\n","    Args:\n","        tiger_problem (TigerProblem): a problem instance\n","        planner (Planner): a planner\n","        nsteps (int): Maximum number of steps to run this loop.\n","        debug_tree (bool): True if get into the pdb with a\n","                           TreeDebugger created as 'dd' variable.\n","    \"\"\"\n","    for i in range(nsteps):\n","        action = planner.plan(tiger_problem.agent) # método plan integrado y pasamos el agente (planificación)\n","        if debug_tree:\n","            from pomdp_py.utils import TreeDebugger\n","            dd = TreeDebugger(tiger_problem.agent.tree)\n","            import pdb; pdb.set_trace()\n","\n","        print(\"==== Step %d ====\" % (i+1))\n","        print(\"True state:\", tiger_problem.env.state)\n","        print(\"Belief:\", tiger_problem.agent.cur_belief)\n","        print(\"Action:\", action)\n","        # There is no state transition for the tiger domain.\n","        # In general, the ennvironment state can be transitioned\n","        # using\n","        #\n","        #   reward = tiger_problem.env.state_transition(action, execute=True)\n","        #\n","        # Or, it is possible that you don't have control\n","        # over the environment change (e.g. robot acting\n","        # in real world); In that case, you could skip\n","        # the state transition and re-estimate the state\n","        # (e.g. through the perception stack on the robot).\n","        reward = tiger_problem.env.reward_model.sample(tiger_problem.env.state, action, None)\n","        print(\"Reward:\", reward)\n","\n","        # Let's create some simulated real observation;\n","        # Here, we use observation based on true state for sanity\n","        # checking solver behavior. In general, this observation\n","        # should be sampled from agent's observation model, as\n","        #\n","        #    real_observation = tiger_problem.agent.observation_model.sample(tiger_problem.env.state, action)\n","        #\n","        # or coming from an external source (e.g. robot sensor\n","        # reading). Note that tiger_problem.env.state stores the\n","        # environment state after action execution.\n","        real_observation = TigerObservation(tiger_problem.env.state.name) # ejecucion\n","        print(\">> Observation:\",  real_observation)\n","        tiger_problem.agent.update_history(action, real_observation) # actualizar la historia\n","\n","        # Update the belief. If the planner is POMCP, planner.update\n","        # also automatically updates agent belief.\n","        planner.update(tiger_problem.agent, action, real_observation) # actualizar el planificador\n","        if isinstance(planner, pomdp_py.POUCT):\n","            print(\"Num sims:\", planner.last_num_sims)\n","            print(\"Plan time: %.5f\" % planner.last_planning_time)\n","\n","        if isinstance(tiger_problem.agent.cur_belief,\n","                      pomdp_py.Histogram):\n","            new_belief = pomdp_py.update_histogram_belief(\n","                tiger_problem.agent.cur_belief,\n","                action, real_observation,\n","                tiger_problem.agent.observation_model,\n","                tiger_problem.agent.transition_model)\n","            tiger_problem.agent.set_belief(new_belief)\n","\n","        if action.name.startswith(\"open\"):\n","            # Make it clearer to see what actions are taken\n","            # until every time door is opened.\n","            print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"id":"217021ea","metadata":{"id":"217021ea"},"outputs":[],"source":["def main():\n","    init_true_state = random.choice([TigerState(\"tiger-left\"),\n","                                     TigerState(\"tiger-right\")])\n","    init_belief = pomdp_py.Histogram({TigerState(\"tiger-left\"): 0.5,\n","                                      TigerState(\"tiger-right\"): 0.5})\n","    tiger_problem = TigerProblem(0.15,  # observation noise\n","                                 init_true_state, init_belief)\n","\n","    print(\"** Testing value iteration **\")\n","    vi = pomdp_py.ValueIteration(horizon=3, discount_factor=0.95)\n","    test_planner(tiger_problem, vi, nsteps=3)\n","\n","    # Reset agent belief\n","    tiger_problem.agent.set_belief(init_belief, prior=True)\n","\n","    print(\"\\n** Testing POUCT **\")\n","    pouct = pomdp_py.POUCT(max_depth=3, discount_factor=0.95,\n","                           num_sims=4096, exploration_const=50,\n","                           rollout_policy=tiger_problem.agent.policy_model,\n","                           show_progress=True)\n","    test_planner(tiger_problem, pouct, nsteps=10)\n","    TreeDebugger(tiger_problem.agent.tree).pp\n","\n","    # Reset agent belief\n","    tiger_problem.agent.set_belief(init_belief, prior=True)\n","    tiger_problem.agent.tree = None\n","\n","    print(\"** Testing POMCP **\")\n","    tiger_problem.agent.set_belief(pomdp_py.Particles.from_histogram(init_belief, num_particles=100), prior=True)\n","    pomcp = pomdp_py.POMCP(max_depth=3, discount_factor=0.95, # instanciar el POMCP funciona por muestreo\n","                           num_sims=1000, exploration_const=50,\n","                           rollout_policy=tiger_problem.agent.policy_model,\n","                           show_progress=True, pbar_update_interval=500)\n","    test_planner(tiger_problem, pomcp, nsteps=10)\n","    TreeDebugger(tiger_problem.agent.tree).pp"]},{"cell_type":"code","execution_count":null,"id":"30052160","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"30052160","outputId":"0c1d992f-07b1-4f76-8f44-8994d25d0136"},"outputs":[{"output_type":"stream","name":"stdout","text":["** Testing value iteration **\n","==== Step 1 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.5, TigerState(tiger-right): 0.5}\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","==== Step 2 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.85, TigerState(tiger-right): 0.15}\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","==== Step 3 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.9697986575573173, TigerState(tiger-right): 0.03020134244268276}\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","\n","** Testing POUCT **\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 2652.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 1 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.5, TigerState(tiger-right): 0.5}\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 4096\n","Plan time: 1.54372\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 2682.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 2 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.85, TigerState(tiger-right): 0.15}\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 4096\n","Plan time: 1.52657\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 3165.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 3 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.9697986575573173, TigerState(tiger-right): 0.03020134244268276}\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 4096\n","Plan time: 1.29360\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 3285.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 4 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.994534412751245, TigerState(tiger-right): 0.005465587248755101}\n","Action: open-right\n","Reward: 10\n",">> Observation: tiger-left\n","Num sims: 4096\n","Plan time: 1.24622\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 3067.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 5 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.5, TigerState(tiger-right): 0.5}\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 4096\n","Plan time: 1.33484\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:02<00:00, 1978.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 6 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.85, TigerState(tiger-right): 0.15}\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 4096\n","Plan time: 2.06991\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 3228.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 7 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.9697986575573173, TigerState(tiger-right): 0.03020134244268276}\n","Action: open-right\n","Reward: 10\n",">> Observation: tiger-left\n","Num sims: 4096\n","Plan time: 1.26845\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 3420.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 8 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.5, TigerState(tiger-right): 0.5}\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 4096\n","Plan time: 1.19701\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 3450.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 9 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.85, TigerState(tiger-right): 0.15}\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 4096\n","Plan time: 1.18682\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 3286.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 10 ====\n","True state: tiger-left\n","Belief: {TigerState(tiger-left): 0.9697986575573173, TigerState(tiger-right): 0.03020134244268276}\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 4096\n","Plan time: 1.24587\n","\u001b[92m_VNodePP\u001b[0m(n=5963, v=7.115)\u001b[96m(depth=0)\u001b[0m\n","├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=1026, v=4.600)\n","│    ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=808, v=8.721)\u001b[96m(depth=1)\u001b[0m\n","│    │    ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=65, v=-3.938)\n","│    │    │    ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=2, v=0.000)\u001b[96m(depth=2)\u001b[0m\n","│    │    ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-100.000)\n","│    │    └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=741, v=8.721)\n","│    │         ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=118, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","│    │         │    ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=115, v=-1.000)\n","│    │         │    ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-100.000)\n","│    │         └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=115, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","│    │              ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=107, v=-1.000)\n","│    │              ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=4, v=-45.000)\n","│    │              └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=4, v=-45.000)\n","│    └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=149, v=2.884)\u001b[96m(depth=1)\u001b[0m\n","│         ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=31, v=-6.302)\n","│         │    ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=6, v=10.000)\u001b[96m(depth=2)\u001b[0m\n","│         │    │    ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-1.000)\n","│         │    │    └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=3, v=10.000)\n","│         │    └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=2, v=0.000)\u001b[96m(depth=2)\u001b[0m\n","│         └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=117, v=2.884)\n","│              ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=23, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","│              │    ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=13, v=-1.000)\n","│              │    ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=3, v=-26.667)\n","│              │    └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=7, v=-21.429)\n","│              └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=22, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","│                   ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=16, v=-1.000)\n","│                   ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=4, v=-45.000)\n","│                   └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-45.000)\n","├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-100.000)\n","└─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=4935, v=7.115)\n","     ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=2158, v=-2.376)\u001b[96m(depth=1)\u001b[0m\n","     │    ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2146, v=-2.376)\n","     │    │    ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=678, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","     │    │    │    ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=464, v=-1.000)\n","     │    │    │    ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-100.000)\n","     │    │    │    └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=212, v=-4.009)\n","     │    │    └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=660, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","     │    │         ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=525, v=-1.000)\n","     │    │         ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=133, v=-6.541)\n","     │    │         └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-100.000)\n","     │    ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-100.000)\n","     │    └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=10, v=-45.000)\n","     └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=2163, v=-2.358)\u001b[96m(depth=1)\u001b[0m\n","          ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2139, v=-2.358)\n","          │    ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=694, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","          │    │    ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=611, v=-1.000)\n","          │    │    ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-100.000)\n","          │    │    └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=81, v=-10.370)\n","          │    └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=659, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","          │         ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=637, v=-1.000)\n","          │         ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=18, v=-26.667)\n","          │         └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=4, v=-72.500)\n","          ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=14, v=-43.929)\n","          └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=10, v=-45.000)\n","** Testing POMCP **\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 4059.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 1 ====\n","True state: tiger-left\n","Belief: [(TigerState(tiger-left), 0.55), (TigerState(tiger-right), 0.45)]\n","Action: open-right\n","Reward: 10\n",">> Observation: tiger-left\n","Num sims: 1000\n","Plan time: 0.23578\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 6412.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 2 ====\n","True state: tiger-left\n","Belief: [(TigerState(tiger-left), 0.5246913580246914), (TigerState(tiger-right), 0.47530864197530864)]\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 1000\n","Plan time: 0.14558\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 3526.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 3 ====\n","True state: tiger-left\n","Belief: [(TigerState(tiger-left), 0.8625954198473282), (TigerState(tiger-right), 0.13740458015267176)]\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 1000\n","Plan time: 0.27468\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 7719.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 4 ====\n","True state: tiger-left\n","Belief: [(TigerState(tiger-right), 0.02194787379972565), (TigerState(tiger-left), 0.9780521262002744)]\n","Action: open-right\n","Reward: 10\n",">> Observation: tiger-left\n","Num sims: 1000\n","Plan time: 0.11923\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 7841.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 5 ====\n","True state: tiger-left\n","Belief: [(TigerState(tiger-right), 0.4759825327510917), (TigerState(tiger-left), 0.5240174672489083)]\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 1000\n","Plan time: 0.12619\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 8033.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 6 ====\n","True state: tiger-left\n","Belief: [(TigerState(tiger-left), 0.8494208494208494), (TigerState(tiger-right), 0.15057915057915058)]\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 1000\n","Plan time: 0.11770\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 8307.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 7 ====\n","True state: tiger-left\n","Belief: [(TigerState(tiger-right), 0.032981530343007916), (TigerState(tiger-left), 0.9670184696569921)]\n","Action: open-right\n","Reward: 10\n",">> Observation: tiger-left\n","Num sims: 1000\n","Plan time: 0.11919\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 4865.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 8 ====\n","True state: tiger-left\n","Belief: [(TigerState(tiger-right), 0.47835051546391755), (TigerState(tiger-left), 0.5216494845360825)]\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 1000\n","Plan time: 0.18071\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 7514.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 9 ====\n","True state: tiger-left\n","Belief: [(TigerState(tiger-right), 0.12133072407045009), (TigerState(tiger-left), 0.8786692759295499)]\n","Action: listen\n","Reward: -1\n",">> Observation: tiger-left\n","Num sims: 1000\n","Plan time: 0.12702\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 7677.83it/s]"]},{"output_type":"stream","name":"stdout","text":["==== Step 10 ====\n","True state: tiger-left\n","Belief: [(TigerState(tiger-left), 0.9749009247027741), (TigerState(tiger-right), 0.02509907529722589)]\n","Action: open-right\n","Reward: 10\n",">> Observation: tiger-left\n","Num sims: 1000\n","Plan time: 0.12843\n","\n","\n","\u001b[92m_VNodePP\u001b[0m(n=950, v=-2.331)\u001b[96m(depth=0)\u001b[0m\n","├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=933, v=-2.331)\n","│    ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=414, v=0.141)\u001b[96m(depth=1)\u001b[0m\n","│    │    ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=378, v=0.141)\n","│    │    │    ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=172, v=6.589)\u001b[96m(depth=2)\u001b[0m\n","│    │    │    │    ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=42, v=-1.000)\n","│    │    │    │    └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=129, v=6.589)\n","│    │    │    └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=56, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","│    │    │         ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=54, v=-1.000)\n","│    │    ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-100.000)\n","│    │    └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=34, v=-15.028)\n","│    │         ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=2, v=0.000)\u001b[96m(depth=2)\u001b[0m\n","│    └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=411, v=0.348)\u001b[96m(depth=1)\u001b[0m\n","│         ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=265, v=0.348)\n","│         │    ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=45, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","│         │    │    ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=43, v=-1.000)\n","│         │    └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=162, v=6.333)\u001b[96m(depth=2)\u001b[0m\n","│         │         ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=41, v=-1.000)\n","│         │         ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=120, v=6.333)\n","│         ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=144, v=-3.762)\n","│         │    ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=18, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","│         │    │    ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=10, v=-1.000)\n","│         │    │    ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-45.000)\n","│         │    │    └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=6, v=-26.667)\n","│         │    └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=18, v=-1.000)\u001b[96m(depth=2)\u001b[0m\n","│         │         ├─── ₀\u001b[92mlisten\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=15, v=-1.000)\n","│         │         ├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-45.000)\n","│         └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-100.000)\n","├─── ₁\u001b[92mopen-left\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=6, v=-76.825)\n","│    └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=2, v=0.000)\u001b[96m(depth=1)\u001b[0m\n","└─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=11, v=-54.423)\n","     ├─── ₀\u001b[91mtiger-left\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=2, v=10.000)\u001b[96m(depth=1)\u001b[0m\n","     └─── ₁\u001b[91mtiger-right\u001b[0m⟶\u001b[92m_VNodePP\u001b[0m(n=4, v=10.000)\u001b[96m(depth=1)\u001b[0m\n","          └─── ₂\u001b[92mopen-right\u001b[0m⟶\u001b[91m_QNodePP\u001b[0m(n=2, v=-92.500)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["main()"]},{"cell_type":"markdown","id":"17da099d","metadata":{"id":"17da099d"},"source":["## Ejercicio\n","\n","* Analiza la página web http://pomdp.org/\n","* Elije un ejemplo sencillo de http://pomdp.org/examples/\n","* Define el POMDP elegido utilizando lo aprendido de [pomdp-py](https://h2r.github.io/pomdp-py/html/)\n","* Resuelve el POMDP utilizando diversos planificadores\n","* Comenta los resultados\n","\n"]},{"cell_type":"markdown","source":["# Michael's 1D maze\n","\n","discount: 0.75\n","\n","values: reward\n","\n","states: left middle right goal\n","\n","actions: w0 e0\n","\n","observations: nothing goal\n","\n","T: w0\n","\n","1.0 0.0 0.0 0.0\n","\n","1.0 0.0 0.0 0.0\n","\n","0.0 0.0 0.0 1.0\n","\n","0.333333 0.333333 0.333333 0.0\n","\n","T: e0\n","\n","0.0 1.0 0.0 0.0\n","\n","0.0 0.0 0.0 1.0\n","\n","0.0 0.0 1.0 0.0\n","\n","0.333333 0.333333 0.333333 0.0\n","\n","O: *\n","\n","1.0 0.0\n","\n","1.0 0.0\n","\n","1.0 0.0\n","\n","0.0 1.0\n","\n","R: * : * : goal : goal 1.0\n","\n","## Dominio\n","\n","En primer lugar, se define el conjunto de estados, las acciones y las observaciones."],"metadata":{"id":"5mfJiyV6pEeU"},"id":"5mfJiyV6pEeU"},{"cell_type":"code","source":["class RobotState(pomdp_py.State):\n","    def __init__(self, name):\n","        self.name = name\n","    def __hash__(self):\n","        return hash(self.name)\n","    def __eq__(self, other):\n","        if isinstance(other, RobotState):\n","            return self.name == other.name\n","        return False\n","    def __str__(self):\n","        return self.name\n","    def __repr__(self):\n","        return \"RobotState(%s)\" % self.name\n","\n","    def other(self):\n","        numero = np.random.randint(3)\n","        if self.name == \"left\":\n","            match numero:\n","                case 0:\n","                    return RobotState(\"middle\")\n","                case 1:\n","                    return RobotState(\"right\")\n","                case 2:\n","                    return RobotState(\"goal\")\n","\n","        if self.name == \"right\":\n","            match numero:\n","                case 0:\n","                    return RobotState(\"left\")\n","                case 1:\n","                    return RobotState(\"middle\")\n","                case 2:\n","                    return RobotState(\"goal\")\n","\n","        if self.name == \"middle\":\n","            match numero:\n","                case 0:\n","                    return RobotState(\"left\")\n","                case 1:\n","                    return RobotState(\"right\")\n","                case 2:\n","                    return RobotState(\"goal\")\n","        else:\n","            match numero:\n","                case 0:\n","                    return RobotState(\"left\")\n","                case 1:\n","                    return RobotState(\"middle\")\n","                case 2:\n","                    return RobotState(\"right\")\n","\n"],"metadata":{"id":"UvQMK9HqC0sV"},"id":"UvQMK9HqC0sV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RobotAction(pomdp_py.Action):\n","    def __init__(self, name):\n","        self.name = name\n","    def __hash__(self):\n","        return hash(self.name)\n","    def __eq__(self, other):\n","        if isinstance(other, RobotAction):\n","            return self.name == other.name\n","        return False\n","    def __str__(self):\n","        return self.name\n","    def __repr__(self):\n","        return \"RobotAction(%s)\" % self.name"],"metadata":{"id":"fQYiPpBfJ52Z"},"id":"fQYiPpBfJ52Z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RobotObservation(pomdp_py.Observation):\n","    def __init__(self, name):\n","        self.name = name\n","    def __hash__(self):\n","        return hash(self.name)\n","    def __eq__(self, other):\n","        if isinstance(other, RobotObservation):\n","            return self.name == other.name\n","        return False\n","    def __str__(self):\n","        return self.name\n","    def __repr__(self):\n","        return \"RobotObservation(%s)\" % self.name"],"metadata":{"id":"SaTIyLl6KB06"},"id":"SaTIyLl6KB06","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Observation model\n","class ObservationModel(pomdp_py.ObservationModel):\n","\n","    def __init__(self, noise=0.15): # ruido de la observación\n","        self.noise = noise\n","\n","\n","    def probability(self, observation, next_state, action):\n","\n","        ''' O: *\n","                    nothing goal\n","            left     1.0   0.0\n","            middle   1.0   0.0\n","            right    1.0   0.0\n","            goal     0.0   1.0\n","        '''\n","\n","        # si la observación es 'nothing' y el estado siguiente no es 'goal'\n","        if observation.name == \"nothing\" and next_state.name != \"goal\":\n","            return 1.0 - self.noise\n","        # si la observación y el siguiente estado son el objetivo\n","        elif observation.name == \"goal\" and next_state.name == \"goal\":\n","            return 1.0 - self.noise\n","        else:\n","            return self.noise\n","\n","    # muestrear una observación de acuerdo a la probabilidad: a partir de la acción realizada y el estado alcanzado, devuelve la observacion\n","    def sample(self, next_state, action):\n","        if action.name == \"w0\" or action.name == \"e0\":\n","            thresh = 1.0 - self.noise\n","        else:\n","            thresh = 0.5\n","\n","        if random.uniform(0,1) < thresh:\n","            return RobotObservation(next_state.name) # devuelve el estado correcto\n","        else:\n","            return RobotObservation(next_state.other().name) # si no se devuelve otro de los estados\n","\n","    def get_all_observations(self):\n","        # devuelve una lista con todas las observaciones\n","        return [RobotObservation(s) for s in {\"nothing\", \"goal\"}]"],"metadata":{"id":"IrRXOGM8KmVl"},"id":"IrRXOGM8KmVl","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transition Model:\n","class TransitionModel(pomdp_py.TransitionModel):\n","\n","    ''' Devuelve la probabilidad de alcanzar un estado al ejecutar una acción en otro estado.'''\n","    # esto es lo que se implementa en el modelo\n","    def probability(self, next_state, state, action):\n","\n","        ''' T: w0\n","            1.0 0.0 0.0 0.0\n","            1.0 0.0 0.0 0.0\n","            0.0 0.0 0.0 1.0\n","            0.333333 0.333333 0.333333 0.0\n","\n","            T: e0\n","            0.0 1.0 0.0 0.0\n","            0.0 0.0 0.0 1.0\n","            0.0 0.0 1.0 0.0\n","            0.333333 0.333333 0.333333 0.0\n","        '''\n","\n","        if action.name == \"w0\":\n","            if state.name == \"left\" and next_state.name == \"left\":\n","                return 1.0 - 1e-9\n","            elif state.name == \"middle\" and next_state.name == \"left\":\n","                return 1.0 - 1e-9\n","            elif state.name == \"right\" and next_state.name == \"goal\":\n","                return 1.0 - 1e-9\n","            elif state.name == \"goal\" and next_state.name != \"goal\":\n","                return 0.333333\n","            else:\n","                return 1e-9\n","        else:\n","            if state.name == \"left\" and next_state.name == \"middle\":\n","                return 1.0 - 1e-9\n","            elif state.name == \"middle\" and next_state.name == \"goal\":\n","                return 1.0 - 1e-9\n","            elif state.name == \"right\" and next_state.name == \"right\":\n","                return 1.0 - 1e-9\n","            elif state.name == \"goal\" and next_state.name != \"goal\":\n","                return 0.333333\n","            else:\n","                return 1e-9\n","\n","    # muestrear: a partir de la acción realizada y el estado en el que está, devuelve un estado al azar\n","    def sample(self, state, action):\n","        if action.name == \"w0\":\n","            match state.name:\n","                case \"left\":\n","                    return RobotState(\"left\")\n","                case \"middle\":\n","                    return RobotState(\"left\")\n","                case \"right\":\n","                    return RobotState(\"goal\")\n","                case \"goal\":\n","                    numero = np.random.randint(3)\n","                    return self.get_all_states()[numero]\n","        elif action.name == \"e0\":\n","            match state.name:\n","                case \"left\":\n","                    return RobotState(\"middle\")\n","                case \"middle\":\n","                    return RobotState(\"goal\")\n","                case \"right\":\n","                    return RobotState(\"right\")\n","                case \"goal\":\n","                    numero = np.random.randint(3)\n","                    return self.get_all_states()[numero]\n","\n","    def get_all_states(self):\n","        \"\"\"Only need to implement this if you're using\n","        a solver that needs to enumerate over the observation space (e.g. value iteration)\"\"\"\n","        return [RobotState(s) for s in {\"left\", \"middle\", \"right\", \"goal\"}]"],"metadata":{"id":"6wOjvb1in9M1"},"id":"6wOjvb1in9M1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reward Model\n","\n","class RewardModel(pomdp_py.RewardModel):\n","    ''' SxA es la función recompensa asociada a cada estado '''\n","\n","    def _reward_func(self, state, action):\n","      ''' R: * : * : goal : goal 1.0 '''\n","      if action.name == \"w0\" or action.name == \"e0\":\n","            if state.name == \"goal\":\n","                return 1\n","            else:\n","                return 0\n","\n","    def sample(self, state, action, next_state):\n","        return self._reward_func(state, action) # devuelve el reward de aplicar al estado una accion"],"metadata":{"id":"uAdgAw8ooCi2"},"id":"uAdgAw8ooCi2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Policy Model\n","class PolicyModel(pomdp_py.RolloutPolicy):\n","    \"\"\"A simple policy model with uniform prior over a\n","       small, finite action space\"\"\"\n","\n","    ACTIONS = [RobotAction(s) for s in {\"w0\", \"e0\"}]\n","\n","\n","    def sample(self, state): # dame una accion al azar aplicable al estado\n","        return random.sample(self.get_all_actions(), 1)[0]\n","\n","\n","    # función que ejecuta acciones hasta llegar a un estado de parada.\n","    def rollout(self, state, history=None):\n","        \"\"\"Treating this PolicyModel as a rollout policy\"\"\"\n","        return self.sample(state)\n","\n","    def get_all_actions(self, state=None, history=None):\n","    # lista de todas las acciones a partir de un estado concreto.\n","        return PolicyModel.ACTIONS"],"metadata":{"id":"oHqwD3OxoIYn"},"id":"oHqwD3OxoIYn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MazeProblem(pomdp_py.POMDP):\n","    \"\"\"\n","    Creación del problema del laberinto.\n","    \"\"\"\n","\n","    def __init__(self, obs_noise, init_true_state, init_belief):\n","        \"\"\"init_belief is a Distribution.\"\"\"\n","        # separamos el agente del entorno\n","        agent = pomdp_py.Agent(init_belief,\n","                               PolicyModel(),\n","                               TransitionModel(),\n","                               ObservationModel(obs_noise),\n","                               RewardModel())\n","        env = pomdp_py.Environment(init_true_state, # le pasamos cual es el estado real\n","                                   TransitionModel(),\n","                                   RewardModel())\n","        super().__init__(agent, env, name=\"MazeProblem\")\n","\n","    @staticmethod\n","    def create(state=\"left\", belief=0.25, obs_noise=0.0):\n","        \"\"\"\n","        Args:\n","            state (str): could be 'left', 'middle', 'right' or 'goal';\n","                         True state of the environment\n","            belief (float): Initial belief that the target is\n","                            on the left; Between 0-1.\n","            obs_noise (float): Noise for the observation\n","                               model\n","        \"\"\"\n","        init_true_state = RobotState(state)\n","        # crea el belief (distribucion de probabilidad)\n","        # cada clave es un estado\n","        init_belief = pomdp_py.Histogram({\n","            RobotState(\"left\"): belief,\n","            RobotState(\"middle\"): belief,\n","            RobotState(\"right\"): belief,\n","            RobotState(\"goal\"): belief\n","        })\n","        maze_problem = MazeProblem(obs_noise,\n","                                     init_true_state, init_belief)\n","        maze_problem.agent.set_belief(init_belief, prior=True)\n","        return maze_problem"],"metadata":{"id":"u2ztXtvUoQ3n"},"id":"u2ztXtvUoQ3n","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_planner2(maze_problem, planner, nsteps=3,\n","                 debug_tree=False):\n","    \"\"\"\n","\n","    Args:\n","        maze_problem (MazeProblem): a problem instance\n","        planner (Planner): a planner\n","        nsteps (int): Maximum number of steps to run this loop.\n","        debug_tree (bool): True if get into the pdb with a\n","                           TreeDebugger created as 'dd' variable.\n","    \"\"\"\n","    for i in range(nsteps):\n","\n","        action = planner.plan(maze_problem.agent) # método plan integrado y pasamos el agente (planificación)\n","        if debug_tree:\n","            from pomdp_py.utils import TreeDebugger\n","            dd = TreeDebugger(maze_problem.agent.tree)\n","            import pdb; pdb.set_trace()\n","\n","        print(\"==== Step %d ====\" % (i+1))\n","        print(\"True state:\", maze_problem.env.state)\n","        print(\"Belief:\", maze_problem.agent.cur_belief)\n","        print(\"Action:\", action)\n","\n","        reward = maze_problem.env.reward_model.sample(maze_problem.env.state, action, None)\n","        print(\"Reward:\", reward)\n","\n","        if maze_problem.env.state.name != \"goal\":\n","            real_observation = RobotObservation(\"nothing\")\n","        else:\n","            real_observation = RobotObservation(\"goal\")\n","\n","        print(\">> Observation:\",  real_observation)\n","        maze_problem.agent.update_history(action, real_observation) # actualizar la historia\n","\n","        # Update the belief. If the planner is POMCP, planner.update\n","        # also automatically updates agent belief.\n","        planner.update(maze_problem.agent, action, real_observation) # actualizar el planificador\n","        if isinstance(planner, pomdp_py.POUCT):\n","            print(\"Num sims:\", planner.last_num_sims)\n","            print(\"Plan time: %.5f\" % planner.last_planning_time)\n","\n","        if isinstance(maze_problem.agent.cur_belief,\n","                      pomdp_py.Histogram):\n","            new_belief = pomdp_py.update_histogram_belief(\n","                maze_problem.agent.cur_belief,\n","                action, real_observation,\n","                maze_problem.agent.observation_model,\n","                maze_problem.agent.transition_model)\n","            maze_problem.agent.set_belief(new_belief)\n","\n"],"metadata":{"id":"SRtzMMjiEa8S"},"id":"SRtzMMjiEa8S","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    init_true_state = random.choice([RobotState(\"left\"),\n","                                     RobotState(\"middle\"),\n","                                     RobotState(\"right\"),\n","                                     RobotState(\"goal\")])\n","    init_belief = pomdp_py.Histogram({RobotState(\"left\"): 0.25,\n","                                      RobotState(\"middle\"): 0.25,\n","                                      RobotState(\"right\"): 0.25,\n","                                      RobotState(\"goal\"): 0.25})\n","    maze_problem = MazeProblem(0.15,  # observation noise\n","                                 init_true_state, init_belief)\n","\n","    print(\"** Testing value iteration **\")\n","    vi = pomdp_py.ValueIteration(horizon=3, discount_factor=0.75)\n","    test_planner2(maze_problem, vi, nsteps=3)\n","\n","    # Reset agent belief\n","    maze_problem.agent.set_belief(init_belief, prior=True)\n","\n","    print(\"\\n** Testing POUCT **\")\n","    pouct = pomdp_py.POUCT(max_depth=3, discount_factor=0.75,\n","                           num_sims=4096, exploration_const=50,\n","                           rollout_policy=maze_problem.agent.policy_model,\n","                           show_progress=True)\n","    test_planner2(maze_problem, pouct, nsteps=10)\n","    #TreeDebugger(maze_problem.agent.tree).pp\n","\n","    # Reset agent belief\n","    maze_problem.agent.set_belief(init_belief, prior=True)\n","    maze_problem.agent.tree = None\n","\n"],"metadata":{"id":"NNYGtWCOFKC7"},"id":"NNYGtWCOFKC7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"id":"3fEznfw9IAaf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4c0c093c-b81b-435f-a96a-8511462e77f2","executionInfo":{"status":"ok","timestamp":1707737219213,"user_tz":-60,"elapsed":27655,"user":{"displayName":"Gabriel ChavesBenítez","userId":"10155876556175710393"}}},"id":"3fEznfw9IAaf","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["** Testing value iteration **\n","==== Step 1 ====\n","True state: middle\n","Belief: {RobotState(left): 0.25, RobotState(middle): 0.25, RobotState(right): 0.25, RobotState(goal): 0.25}\n","Action: e0\n","Reward: 0\n",">> Observation: nothing\n","==== Step 2 ====\n","True state: middle\n","Belief: {RobotState(left): 0.10493820047037097, RobotState(middle): 0.41975311323343323, RobotState(right): 0.41975311323343323, RobotState(goal): 0.055555573062762695}\n","Action: e0\n","Reward: 0\n",">> Observation: nothing\n","==== Step 3 ====\n","True state: middle\n","Belief: {RobotState(left): 0.028301872132445612, RobotState(middle): 0.18867914017227913, RobotState(right): 0.6698114201316056, RobotState(goal): 0.11320756756366976}\n","Action: w0\n","Reward: 0\n",">> Observation: nothing\n","\n","** Testing POUCT **\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 2737.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 1 ====\n","True state: middle\n","Belief: {RobotState(left): 0.25, RobotState(middle): 0.25, RobotState(right): 0.25, RobotState(goal): 0.25}\n","Action: e0\n","Reward: 0\n",">> Observation: nothing\n","Num sims: 4096\n","Plan time: 1.49552\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 3020.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 2 ====\n","True state: middle\n","Belief: {RobotState(left): 0.10493820047037097, RobotState(middle): 0.41975311323343323, RobotState(right): 0.41975311323343323, RobotState(goal): 0.055555573062762695}\n","Action: e0\n","Reward: 0\n",">> Observation: nothing\n","Num sims: 4096\n","Plan time: 1.35578\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 3005.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 3 ====\n","True state: middle\n","Belief: {RobotState(left): 0.028301872132445612, RobotState(middle): 0.18867914017227913, RobotState(right): 0.6698114201316056, RobotState(goal): 0.11320756756366976}\n","Action: w0\n","Reward: 0\n",">> Observation: nothing\n","Num sims: 4096\n","Plan time: 1.36235\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 2924.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 4 ====\n","True state: middle\n","Belief: {RobotState(left): 0.5680692161925636, RobotState(middle): 0.08415838502344214, RobotState(right): 0.08415838502344214, RobotState(goal): 0.2636140137605523}\n","Action: e0\n","Reward: 0\n",">> Observation: nothing\n","Num sims: 4096\n","Plan time: 1.40030\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 2829.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 5 ====\n","True state: middle\n","Belief: {RobotState(left): 0.09441487862831364, RobotState(middle): 0.7047872758885069, RobotState(right): 0.18484040029377927, RobotState(goal): 0.01595744518940011}\n","Action: e0\n","Reward: 0\n",">> Observation: nothing\n","Num sims: 4096\n","Plan time: 1.44696\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:04<00:00, 981.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 6 ====\n","True state: middle\n","Belief: {RobotState(left): 0.01267709647190665, RobotState(middle): 0.2376957288972253, RobotState(right): 0.4532065415691509, RobotState(goal): 0.2964206330617172}\n","Action: w0\n","Reward: 0\n",">> Observation: nothing\n","Num sims: 4096\n","Plan time: 4.17082\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 2708.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 7 ====\n","True state: middle\n","Belief: {RobotState(left): 0.5571089316740614, RobotState(middle): 0.15764420004570753, RobotState(right): 0.15764420004570753, RobotState(goal): 0.12760266823452351}\n","Action: e0\n","Reward: 0\n",">> Observation: nothing\n","Num sims: 4096\n","Plan time: 1.51169\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 2517.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 8 ====\n","True state: middle\n","Belief: {RobotState(left): 0.04888001773354327, RobotState(middle): 0.68910615379413, RobotState(right): 0.23004375668913007, RobotState(goal): 0.03197007178319658}\n","Action: e0\n","Reward: 0\n",">> Observation: nothing\n","Num sims: 4096\n","Plan time: 1.62633\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:01<00:00, 2289.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==== Step 9 ====\n","True state: middle\n","Belief: {RobotState(left): 0.024639679925390012, RobotState(middle): 0.13765685665671198, RobotState(right): 0.5565317927187788, RobotState(goal): 0.281171670699119}\n","Action: w0\n","Reward: 0\n",">> Observation: nothing\n","Num sims: 4096\n","Plan time: 1.78885\n"]},{"output_type":"stream","name":"stderr","text":["100%|█████████▉| 4095/4096 [00:02<00:00, 1763.55it/s]"]},{"output_type":"stream","name":"stdout","text":["==== Step 10 ====\n","True state: middle\n","Belief: {RobotState(left): 0.47264179453044924, RobotState(middle): 0.17302447469294668, RobotState(right): 0.17302447469294668, RobotState(goal): 0.18130925608365742}\n","Action: e0\n","Reward: 0\n",">> Observation: nothing\n","Num sims: 4096\n","Plan time: 2.32191\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}